library(KoNLP)
install.packages("Rtools")
library(httr)
library(rvest)
library(stringr)
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(KoNLP)
# 앞에 붙어있는 태그를 빼는 함수
parse_str <- function(s) {
if (startsWith(s, "[건설뉴스]")) {
str <- substring(s, 8)
} else if (startsWith(s, "[뉴스]") | startsWith(s, "[인사]") | startsWith(s, "[정부]")) {
str <- substring(s, 6)
} else if (startsWith(s, "[기술인 신간 소개]")){
str <- substring(s, 13)
} else {
str <- s
}
return (str)
}
url <- "http://www.gisulin.kr/news/index.html?section=1&category=3"
# 이상하게 rvest의 read_html을 쓰면 원본하고 다른 html이 읽혀서 이상한 결과가 나온다.
# 그래서 다른 방법으로 html 파일을 읽어왔다.
response <- GET(url=url, encode = "form")
titles <- response %>%
as.character() %>%
read_html %>%
html_nodes(xpath='//*[@id="sect3_articlelist"]') %>%
html_nodes("dl") %>%
html_nodes("dt") %>%
html_text() %>%
data.frame()
for (i in 2:10) {
new_url <- str_c(url, "&page=", i)
print(new_url)
res <- POST(url=new_url, encode="form")
title <- res %>%
as.character() %>%
read_html() %>%
html_nodes(xpath='//*[@id="sect3_articlelist"]') %>%
html_nodes("dl") %>%
html_nodes("dt") %>%
html_text() %>%
data.frame()
titles <- rbind(titles, title)
}
class(titles)
titles <- apply(titles, 1, parse_str)
head(titles)
useNIADic()
apply(titles, 1, extractNoun)
class(titles)
head(titles)
data.frame(titles)
titles <- data.frame(titles)
apply(titles, 1, extractNoun)
nouns <- apply(titles, 1, extractNoun)
table(unlist(nouns))
unlist(nouns)
table(unlist(nouns))
nouns
# 분석 과정
doc <- VectorSource(as.matrix(titles)) %>% Corpus
inspect(doc)
# 사실 이들 대부분은 영어를 위한 것들이라 불필요해 보이지만
# 그래도 텍스트 중 영어가 아예 없는 건 아니라서 tm_map을 활용했다.
doc <- tm_map(doc, content_transformer(tolower))
doc <- tm_map(doc, removeNumbers)
doc <- tm_map(doc, removeWords, stopwords("english"))
doc <- tm_map(doc, removePunctuation)
doc <- tm_map(doc, stripWhitespace)
class(doc)
extractNoun(doc)
nouns <- extractNoun(doc)
table(unlist(nouns))
wordcount <- table(unlist(nouns))
df_words <- data.frame(wordcount)
df_words
df_words <- rename(df_words,
word = Var1,
freq = Freq)
wordcount
df_words <- data.frame(wordcount)
df_words
sort(df_words, decreasing=TRUE)
sort(df_words[,2], decreasing=TRUE)
df_words[order(df_words$Freq),]
df_words[order(df_words$Freq, decreasing = TRUE),]
df_words <- df_words[order(df_words$Freq, decreasing = TRUE),]
filter(df_words, nchar(Var1) >= 2)
filter(df_words, nchar(word) >= 2)
df_words[length(df_words$Var1) >= 2, ]
df_words[length(df_words$Var1) > 2, ]
head(df_words)
df_words[1, 1]
length(df_words[1, 1])
length(df_words$Var1) == 1
apply(df_words, 1, length) == 1
apply(df_words, 1, length)
head(df_words)
apply(df_words$Var1, 1, length)
length(df_words[,1]) == 1
length(df_words[1,1]) == 1
df_words[nchar(df_words$Var1) >= 2,]
df_words$Var1
class(df_words$Var1)
c(df_words$Var1)
filter(df_words, nchar(Var1) >= 2)
filter(df_words, nchar(word) >= 2)
df_words <- as.data.frame(wordcount, stringsAsFactors = F)
df_words <- df_words[order(df_words$Freq, decreasing = TRUE),]
head(df_words)
length(df_words[1,1]) == 1
filter(df_words, nchar(word) >= 2)
df_words[nchar(df_words$Var1) >= 2,]
df_words <- df_words[nchar(df_words$Var1) >= 2,]
word_cloud(df_words)
word_cloud2(df_words)
wordcloud2(df_words)
library(wordcl)
library(wordcloud2)
wordcloud2(df_words)
library(httr)
library(rvest)
library(stringr)
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(KoNLP)
library(wordcloud2)
# 앞에 붙어있는 태그를 빼는 함수
parse_str <- function(s) {
if (startsWith(s, "[건설뉴스]")) {
str <- substring(s, 8)
} else if (startsWith(s, "[뉴스]") | startsWith(s, "[인사]") | startsWith(s, "[정부]")) {
str <- substring(s, 6)
} else if (startsWith(s, "[기술인 신간 소개]")){
str <- substring(s, 13)
} else {
str <- s
}
return (str)
}
url <- "http://www.gisulin.kr/news/index.html?section=1&category=3"
# 이상하게 rvest의 read_html을 쓰면 원본하고 다른 html이 읽혀서 이상한 결과가 나온다.
# 그래서 다른 방법으로 html 파일을 읽어왔다.
response <- GET(url=url, encode = "form")
titles <- response %>%
as.character() %>%
read_html %>%
html_nodes(xpath='//*[@id="sect3_articlelist"]') %>%
html_nodes("dl") %>%
html_nodes("dt") %>%
html_text() %>%
data.frame()
for (i in 2:10) {
new_url <- str_c(url, "&page=", i)
print(new_url)
res <- POST(url=new_url, encode="form")
title <- res %>%
as.character() %>%
read_html() %>%
html_nodes(xpath='//*[@id="sect3_articlelist"]') %>%
html_nodes("dl") %>%
html_nodes("dt") %>%
html_text() %>%
data.frame()
titles <- rbind(titles, title)
}
class(titles)
titles <- apply(titles, 1, parse_str)
titles <- data.frame(titles)
# 분석 과정
doc <- VectorSource(as.matrix(titles)) %>% Corpus
inspect(doc)
doc <- tm_map(doc, content_transformer(tolower))
doc <- tm_map(doc, removeNumbers)
doc <- tm_map(doc, removeWords, stopwords("english"))
doc <- tm_map(doc, removePunctuation)
doc <- tm_map(doc, stripWhitespace)
dtm <- DocumentTermMatrix(doc)
inspect(dtm)
m <- as.matrix(dtm)
v <- sort(colSums(m), decreasing = T)
d <- data.frame(word=names(v), freq=v)
pal <- brewer.pal(9, "Set1")
wordcloud(words=d$word,
freq=d$freq,
min.freq = 1,
max.words = 100,
random.order=F,
rot.per=0.35,
color=pal)
findAssocs(dtm, terms="코로나", corlimit=0.5)
barplot(d[1:10,]$freq,
las=2,
names.arg=d[1:10,]$word,
col="grey",
main="Top 10 Frequent Words",
ylab="Frequency")
# Using KoNLP
useNIADic()
nouns <- extractNoun(doc)
wordcount <- table(unlist(nouns))
df_words <- as.data.frame(wordcount, stringsAsFactors = F)
# 내림차순 정렬
df_words <- df_words[order(df_words$Freq, decreasing = TRUE),]
head(df_words)
# 한글자 단어들은 제거했다
df_words <- df_words[nchar(df_words$Var1) >= 2,]
wordcloud2(df_words)
library(httr)
library(rvest)
library(stringr)
library(KoNLP)
library(RSelenium)
# helper method
# --------------------------------------
parse_string <- function(s) {
s <- str_replace_all(s, "[\t\n]", "")
s <- str_replace(s, "^\\s+", "")
return (s)
}
parse_date <- function(s) {
s <- str_replace_all(s, "\\.", "-")
s <- as.character(s)
return (s)
}
parse_price <- function(s) {
s <- str_replace_all(s, ",", "")
s <- as.integer(s)
return (s)
}
Sys.Date()
# SK 이노베이션 사이트 URL
url <- "http://www.skinnovation.com/company/press.asp"
# RSelenium 구동을 위한 port열기
remDr <- remoteDriver(port=4445L, browserName="chrome")
# browser 오픈
remDr$open()
remDr$navigate(url)
# 날짜와 헤드라인 저장할 벡터 생성
dates <- c()
titles <- c()
# 퍼오기
for (i in 1:4) {
for (j in 4:13) {
html <- remDr$getPageSource()[[1]]
table <- html %>%
read_html() %>%
html_nodes("table")
tds <- table %>%
html_nodes("td")
date <- c()
for (i in 1:10) {
date[i] <- tds[i * 3] %>%
html_text()
}
dates <- c(dates, date)
titles <- c(titles, table %>%
html_nodes("a") %>%
html_text())
# 다음 페이지 버튼 찾아서 클릭
nextbutton <- remDr$findElement(using="xpath", str_c('//*[@id="contents"]/div/div[3]/div[2]/a[', j, ']'))
nextbutton$clickElement()
}
}
for (k in 4:9) {
html <- remDr$getPageSource()[[1]]
table <- html %>%
read_html() %>%
html_nodes("table")
tds <- table %>%
html_nodes("td")
date <- c()
for (i in 1:10) {
date[i] <- tds[i * 3] %>%
html_text()
}
dates <- c(dates, date)
titles <- c(titles, table %>%
html_nodes("a") %>%
html_text())
nextbutton <- remDr$findElement(using="xpath", str_c('//*[@id="contents"]/div/div[3]/div[2]/a[', k, ']'))
nextbutton$clickElement()
}
google_url <- "https://www.google.com/"
remDr$navigate(google_url)
searchElem <- remDr$findElement(using="xpath", '//*[@id="tsf"]/div[2]/div[1]/div[1]/div/div[2]/input')
searchElem$sendKeysToElement(list("SK이노베이션", key="enter"))
newsbutton <- remDr$findElement(using="xpath", '//*[@id="hdtb-msb-vis"]/div[2]/a')
newsbutton$clickElement()
headlines <- c()
news_dates <- c()
for (num in 1:24) {
if (num %% 10 == 0) {
Sys.sleep(2)
}
html <- remDr$getPageSource()[[1]]
search_results <- html %>%
read_html() %>%
html_nodes("div.g")
headline <- search_results %>%
html_nodes("a.l.lLrAF") %>%
html_text()
for (i in 1:length(search_results)) {
ndate <- search_results[i] %>%
html_nodes("span.f.nsa.fwzPFf")
news_date <- ndate[1] %>%
html_text()
news_dates <- c(news_dates, news_date)
}
headlines <- c(headlines, headline)
if (num != 24) {
nbutton <- remDr$findElement(using="xpath", '//*[@id="pnnext"]')
nbutton$clickElement()
}
}
google_news <- data.frame(news_dates, headlines)
# 뉴스 dataframe 생성 및 저장
SK_news <- data.frame(dates, titles)
# 주식 데이터 저장 공간 생성
stock_dates <- c()
end_price <- c()
start_price <- c()
high_price <- c()
low_price <- c()
amount <- c()
# URL 마다 스크레이핑
for (pgNum in 1:232) {
stock_url <- str_c("https://finance.naver.com/item/sise_day.nhn?code=096770&page=", pgNum)
remDr$navigate(stock_url)
html <- remDr$getPageSource()[[1]]
trs <- html %>%
read_html() %>%
html_nodes("tr")
for (i in trs[c(3:7, 11:(length(trs) - 2))]) {
values <- i %>%
html_nodes("span") %>%
html_text()
stock_dates <- c(stock_dates, values[1])
end_price <- c(end_price, values[2])
start_price <- c(start_price, values[4])
high_price <- c(high_price, values[5])
low_price <- c(low_price, values[6])
amount <- c(amount, values[7])
}
}
# 주식 데이터 dataframe 생성
stock_df <- data.frame(stock_dates, end_price, start_price, high_price, low_price, amount)
# 데이터 정제
SK_news <- apply(SK_news, 2, parse_string)
stock_df <- apply(stock_df, 2, parse_date)
stock_prices <- apply(stock_df[, 2:6], 2, parse_price)
stock_df <- data.frame(stock_df[, 1], stock_prices)
colnames(stock_df)[1] <- "dates"
stock_df[, 1] <- as.character(stock_df[, 1])
SK_news <- data.frame(SK_news)
SK_news[, 1] <- as.character(SK_news[, 1])
# 두 개의 dataframe을 날짜를 기준으로 병합
df <- merge(SK_news, stock_df, by="dates" , all=T)
# csv파일 생성
write.csv(df, file="SK이노베이션.csv", row.names=F)
write.csv(google_news, file="google_news.csv", row.names=F)
library(RSelenium)
library(httr)
library(rvest)
library(stringr)
url <- "http://find.mk.co.kr/new/search.php?pageNum=1&cat=&cat1=&media_eco=&pageSize=20&sub=vod&dispFlag=OFF&page=vod&s_kwd=%B4%EB%BF%EC%B0%C7%BC%B3&s_page=total&go_page=&ord=1&ord1=1&ord2=0&s_keyword=%B4%EB%BF%EC%B0%C7%BC%B3&y1=1991&m1=01&d1=01&y2=2020&m2=06&d2=04&area=ttbd"
remDr <- remoteDriver(port=4445L, browserName="chrome")
remDr$open()
remDr$navigate(url=url)
moreButton <- remDr$findElement('xpath', '/html/body/center/div[1]/div[7]/a')
library(KoNLP)
library(data.table)
library(tm)
library(SnowballC)
library(wordcloud2)
library(dplyr)
df <- fread("현대자동차_한경.csv", encoding = "UTF-8")
setwd("C:/Users/kwon/Desktop/R/2020-ERICA-Construction-Programming-R/Team Project/hyundai_car")
df <- fread("현대자동차_한경.csv", encoding = "UTF-8")
doc <- VectorSource(df$news_title) %>% Corpus
inspect(doc)
head(df)
df <- fread("현대자동차_한경.csv", encoding = "euc-kr")
df <- read.csv("현대자동차_한경.csv")
head(df)
doc <- VectorSource(df$news_title) %>% Corpus
inspect(doc)
doc <- tm_map(doc, removePunctuation)
doc <- tm_map(doc, removeWords, stopwords("english"))
doc <- tm_map(doc, tolower)
doc <- VectorSource(df$news_title) %>% VCorpus
doc <- tm_map(doc, removePunctuation)
doc <- tm_map(doc, removeWords, stopwords("english"))
doc <- tm_map(doc, tolower)
inspect(doc)
useNIADic()
nouns <- extractNoun(doc)
wordcount <- table(unlist(nouns))
df_words <- as.data.frame(wordcount, stringsAsFactors = F)
head(df)
doc <- VectorSource(df$titles) %>% Corpus
inspect(doc)
doc <- tm_map(doc, removePunctuation)
doc <- tm_map(doc, removeWords, stopwords("english"))
doc <- tm_map(doc, tolower)
nouns <- extractNoun(doc)
wordcount <- table(unlist(nouns))
df_words <- as.data.frame(wordcount, stringsAsFactors = F)
df_words <- df_words[order(df_words$Freq, decreasing = T),]
df_words <- df_words[nchar(df_words$Var1) >=2, ]
wordcloud2(df_words, gridSize = 15)
wordcloud2(df_words, gridSize = 20)
library(data.table)
library(ggplot2)
library(scales)
df <- fread("삼성물산.csv")
df$dates <- as.Date(df$dates)
ggplot(data=df, aes(x=dates, y=end_prices, group=1)) +
geom_line() +
scale_x_date(date_labels="%Y-%b-%d")
setwd("C:/Users/kwon/Desktop/R/2020-ERICA-Construction-Programming-R/Team Project/samsung_cnt")
library(data.table)
library(ggplot2)
library(scales)
df <- fread("삼성물산.csv")
df$dates <- as.Date(df$dates)
ggplot(data=df, aes(x=dates, y=end_prices, group=1)) +
geom_line() +
scale_x_date(date_labels="%Y-%b-%d")
library(httr)
library(rvest)
library(stringr)
library(dplyr)
parse_string <- function(s) {
s <- str_replace_all(s, "[\t\n\r]", "")
s <- str_replace_all(s, "▶", "")
s <- str_replace_all(s, "◆", "")
s <- str_replace_all(s, "◇", "")
s <- str_replace_all(s, "▲", "")
s <- str_replace_all(s, "■", "")
s <- str_replace_all(s, "△", "")
s <- str_replace(s, "^\\.\\.\\.", "")
s <- str_replace(s, "\\.\\.\\.$", "")
s <- str_replace_all(s, "[\U4E00-\U9FFF\U3000-\U303F]", "")
s <- str_replace_all(s, "^\\s+", "")
s <- str_replace_all(s, "[ ]+$", "")
return (s)
}
parse_date <- function(s) {
s <- substring(s, 1, 10)
s <- str_replace_all(s, "\\.", "-")
s <- as.character(s)
return (s)
}
keywords <- c("삼성물산", "래미안", "건설", "건축", "토목", "반포아파트 3주구", "반포아파트", "반포3주구", "반포", "수주", "분양", "빌딩", "플랜트", "주택")
titles <- c()
dates <- c()
paragraphs <- c()
for (pgNum in 1:800) {
if (pgNum %% 100 == 0) {
print("==============================")
}
url <- str_c("https://search.hankyung.com/apps.frm/search.news?query=%EC%82%BC%EC%84%B1%EB%AC%BC%EC%82%B0&page=", pgNum)
response <- GET(url=url, encode="form")
html <- response %>%
as.character() %>%
read_html()
search_results <- html %>%
html_nodes("ul.article") %>%
html_nodes("li")
all_titles <- search_results %>%
html_nodes("em.tit") %>%
html_text()
all_dates <- search_results %>%
html_nodes("span.date_time") %>%
html_text()
all_paragraphs <- search_results %>%
html_nodes("p.txt") %>%
html_text()
for (i in 1:length(all_titles)) {
values_t <- c()
values_p <- c()
for (word in keywords) {
value_t <- str_detect(all_titles[i], word)
value_p <- str_detect(all_paragraphs[i], word)
values_t <- c(values_t, value_t)
values_p <- c(values_p, value_p)
}
if (sum(values_t) != 0 | sum(values_p) != 0) {
titles <- c(titles, all_titles[i])
dates <- c(dates, all_dates[i])
paragraphs <- c(paragraphs, all_paragraphs[i])
}
}
}
length(titles)
length(dates)
length(paragraphs)
head(titles)
df <- data.frame(dates, titles, paragraphs, stringsAsFactors = F)
df[, 2:3] <- apply(df[, 2:3], 2, parse_string)
head(df)
df$dates <- unlist(lapply(df$dates, parse_date))
head(df)
write.csv(df, "삼성물산_한경.csv", row.names = F)
